{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary package \n",
    "# Make sure to pip install it in Anaconda, if you don't have it\n",
    "import yfinance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignoring warning messages\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  4 of 4 completed\n"
     ]
    }
   ],
   "source": [
    "# Using the .download() method to get our data\n",
    "\n",
    "raw_data = yfinance.download (tickers = \"^GSPC ^FTSE ^N225 ^GDAXI\", #The time series we are interested in - (in our case, these are the S&P, FTSE, NIKKEI and DAX)\n",
    "                              start = \"1994-01-07\", #The starting date of our data set\n",
    "                              end = \"2020-01-27\", #The ending date of our data set (at the time of upload, this is the current date)\n",
    "                              interval = \"1d\", #The distance in time between two recorded observations. Since we're using daily closing prices, we set it equal to \"1d\", which indicates 1 day. \n",
    "                              group_by = 'ticker', #The way we want to group the scraped data. Usually we want it to be \"ticker\", so that we have all the information about a time series in 1 variable.\n",
    "                              auto_adjust = True, #Automatically adjuss the closing prices for each period. \n",
    "                              treads = True) #Whether to use threads for mass downloading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a back up copy in case we remove/alter elements of the data by mistake\n",
    "df_comp = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new columns to the data set\n",
    "df_comp['spx'] = df_comp['^GSPC'].Close\n",
    "df_comp['dax'] = df_comp['^GDAXI'].Close\n",
    "df_comp['ftse'] = df_comp['^FTSE'].Close\n",
    "df_comp['nikkei'] = df_comp['^N225'].Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_comp = df_comp.iloc[1:] # Removing the first elements, since we always start 1 period before the first, due to time zone differences of closing prices\n",
    "del df_comp['^N225']  # Removing the original tickers of the data set\n",
    "del df_comp['^GSPC']\n",
    "del df_comp['^GDAXI']\n",
    "del df_comp['^FTSE']\n",
    "df_comp=df_comp.asfreq('b') # Setting the frequency of the data\n",
    "df_comp=df_comp.fillna(method='ffill') # Filling any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   spx          dax         ftse        nikkei\n",
      "                                                              \n",
      "Date                                                          \n",
      "1994-01-07  469.899994  2224.949951  3446.000000  18124.009766\n",
      "1994-01-10  475.269989  2225.000000  3440.600098  18443.439453\n",
      "1994-01-11  474.130005  2228.100098  3413.800049  18485.250000\n",
      "1994-01-12  474.170013  2182.060059  3372.000000  18793.880859\n",
      "1994-01-13  472.470001  2142.370117  3360.000000  18577.259766\n",
      "                    spx           dax         ftse        nikkei\n",
      "                                                                \n",
      "Date                                                            \n",
      "2020-01-20  3329.620117  13548.940430  7651.399902  24083.509766\n",
      "2020-01-21  3320.790039  13555.870117  7610.700195  23864.560547\n",
      "2020-01-22  3321.750000  13515.750000  7571.899902  24031.349609\n",
      "2020-01-23  3325.540039  13388.419922  7507.700195  23795.439453\n",
      "2020-01-24  3295.469971  13576.679688  7586.000000  23827.179688\n"
     ]
    }
   ],
   "source": [
    "print (df_comp.head()) # Displaying the first 5 elements to make sure the data was scrapped correctly\n",
    "print (df_comp.tail()) # Making sure of the last day we're including in the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
